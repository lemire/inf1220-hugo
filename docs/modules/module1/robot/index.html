<!doctype html><html lang=fr dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Robot conversationnel et intelligence artificielle
  #

L&rsquo;intelligence artificielle (IA) transforme profondément le domaine de la programmation, redéfinissant la manière dont les développeurs conçoivent, écrivent et maintiennent le code. Les outils basés sur l&rsquo;IA, comme les assistants de codage (par exemple, GitHub Copilot), permettent d&rsquo;automatiser des tâches répétitives, telles que la génération de code boilerplate ou la complétion automatique de fonctions. Ces outils s&rsquo;appuient sur des modèles de langage avancés, entraînés sur d&rsquo;immenses bases de données de code, pour proposer des suggestions contextuelles précises. Cette assistance accélère le processus de développement, permettant aux programmeurs de se concentrer sur des aspects plus créatifs et complexes de leurs projets, tout en réduisant les erreurs humaines."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://lemire.github.io/inf1220-hugo/docs/modules/module1/robot/"><meta property="og:site_name" content="INF 1220 - Introduction à la programmation"><meta property="og:title" content="Robot conversationnel et intelligence artificielle"><meta property="og:description" content="Robot conversationnel et intelligence artificielle # L’intelligence artificielle (IA) transforme profondément le domaine de la programmation, redéfinissant la manière dont les développeurs conçoivent, écrivent et maintiennent le code. Les outils basés sur l’IA, comme les assistants de codage (par exemple, GitHub Copilot), permettent d’automatiser des tâches répétitives, telles que la génération de code boilerplate ou la complétion automatique de fonctions. Ces outils s’appuient sur des modèles de langage avancés, entraînés sur d’immenses bases de données de code, pour proposer des suggestions contextuelles précises. Cette assistance accélère le processus de développement, permettant aux programmeurs de se concentrer sur des aspects plus créatifs et complexes de leurs projets, tout en réduisant les erreurs humaines."><meta property="og:locale" content="fr"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Robot conversationnel et intelligence artificielle | INF 1220 - Introduction à la programmation</title><link rel=icon href=/inf1220-hugo/favicon.png><link rel=manifest href=/inf1220-hugo/manifest.json><link rel=canonical href=https://lemire.github.io/inf1220-hugo/docs/modules/module1/robot/><link rel=stylesheet href=/inf1220-hugo/book.min.f5b6812da8cb28cdd091634583a6182b2e7f70231c885158fda89c58af6e3024.css integrity="sha256-9baBLajLKM3QkWNFg6YYKy5/cCMciFFY/aicWK9uMCQ=" crossorigin=anonymous><script defer src=/inf1220-hugo/fuse.min.js></script><script defer src=/inf1220-hugo/fr.search.min.4e7c10a753c95ea164d34814b20bc5db8cbd2a3832ab729435094c8ba692c7b9.js integrity="sha256-TnwQp1PJXqFk00gUsgvF24y9Kjgyq3KUNQlMi6aSx7k=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/inf1220-hugo/><img src=/inf1220-hugo/livre.jpg alt=Logo class=book-icon><span>INF 1220 - Introduction à la programmation</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Rechercher aria-label=Rechercher maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/inf1220-hugo/docs/environnement/>Programmation Java en ligne</a></li><li><a href=/inf1220-hugo/docs/erreurs/>Vous avez trouvé une erreur?</a></li><li><input type=checkbox id=section-5cc50cb3031bd6bd7b763bb20cfb7f0e class=toggle>
<label for=section-5cc50cb3031bd6bd7b763bb20cfb7f0e class=flex><a role=button class=flex-auto>Autres ressources</a></label><ul><li><a href=/inf1220-hugo/docs/extra/credits/>Le professeur</a></li><li><a href=/inf1220-hugo/docs/extra/feuille-de-route/>Feuille de route</a></li><li><a href=/inf1220-hugo/docs/extra/ressources/>Ressources</a></li><li><a href=/inf1220-hugo/docs/extra/faq/>FAQ</a></li><li><a href=/inf1220-hugo/docs/extra/manuel/>Manuel</a></li><li><a href=/inf1220-hugo/docs/extra/math/>Rappel mathématique</a></li><li><a href=/inf1220-hugo/docs/extra/ia/>Intelligence artificielle</a></li><li><a href=/inf1220-hugo/docs/extra/courriel/>Petit guide d’usage du courriel efficace</a></li></ul></li><li><a href=/inf1220-hugo/docs/evaluation/>Évaluation</a></li><li><a href=/inf1220-hugo/docs/pensebete/>Pense-bête java</a></li><li><a href=/inf1220-hugo/docs/modules/>Modules</a><ul><li><input type=checkbox id=section-6ffbdc93c5edebe8ae6289c754c6323e class=toggle checked>
<label for=section-6ffbdc93c5edebe8ae6289c754c6323e class=flex><a href=/inf1220-hugo/docs/modules/module1/ class=flex-auto>Module 1: Algorithme et pseudocode</a></label><ul><li><a href=/inf1220-hugo/docs/modules/module1/teluq/>Modèle du cours</a></li><li><a href=/inf1220-hugo/docs/modules/module1/robot/ class=active>Robot conversationnel et intelligence artificielle</a></li><li><a href=/inf1220-hugo/docs/modules/module1/autoevaluation/>Autoévaluation</a></li><li><a href=/inf1220-hugo/docs/modules/module1/pasapas/>Java pas à pas</a></li><li><a href=/inf1220-hugo/docs/modules/module1/ordinateurs/>Les ordinateurs et leurs langages</a></li><li><a href=/inf1220-hugo/docs/modules/module1/algorithmes/>Les algorithmes</a></li><li><a href=/inf1220-hugo/docs/modules/module1/algorithmes2/>Les algorithmes : conception et syntaxe</a></li><li><a href=/inf1220-hugo/docs/modules/module1/algorithmes3/>Les algorithmes: les structures de contrôle</a></li><li><a href=/inf1220-hugo/docs/modules/module1/difficile/>Les problèmes difficiles</a></li><li><a href=/inf1220-hugo/docs/modules/module1/complex/>Complexité algorithmique</a></li><li><a href=/inf1220-hugo/docs/modules/module1/erreurs/>Les erreurs communes</a></li><li><a href=/inf1220-hugo/docs/modules/module1/exercices/>Exercices sur les algorithmes</a></li><li><a href=/inf1220-hugo/docs/modules/module1/travail-note-1/>Travail noté 1</a></li></ul></li><li><input type=checkbox id=section-71b9dfc90da9a83f173743029715be7a class=toggle>
<label for=section-71b9dfc90da9a83f173743029715be7a class=flex><a href=/inf1220-hugo/docs/modules/module2/ class=flex-auto>Module 2: Introduction au langage Java</a></label><ul><li><a href=/inf1220-hugo/docs/modules/module2/preparation/>Préparation de l’espace de travail</a></li><li><a href=/inf1220-hugo/docs/modules/module2/oriente/>Création d'une classe en Java</a></li><li><a href=/inf1220-hugo/docs/modules/module2/pasapas/>Java pas à pas</a></li><li><a href=/inf1220-hugo/docs/modules/module2/typeoperateur/>Introduction aux types de base et à leurs opérateurs</a></li><li><a href=/inf1220-hugo/docs/modules/module2/exercices-2-1/>Exercices sur les classes, les variables, les types et les opérateurs</a></li><li><a href=/inf1220-hugo/docs/modules/module2/methodes/>Méthodes et constructeurs</a></li><li><a href=/inf1220-hugo/docs/modules/module2/exercices-2-2/>Exercices sur les classes et méthodes</a></li><li><a href=/inf1220-hugo/docs/modules/module2/conseils/>Recommandations</a></li><li><a href=/inf1220-hugo/docs/modules/module2/travail-note-2/>Travail noté 2</a></li></ul></li><li><input type=checkbox id=section-67896e9afc9b4080d8c27b998062dd91 class=toggle>
<label for=section-67896e9afc9b4080d8c27b998062dd91 class=flex><a href=/inf1220-hugo/docs/modules/module3/ class=flex-auto>Module 3: Les structures de données, de contrôle et d'itération en Java</a></label><ul><li><a href=/inf1220-hugo/docs/modules/module3/pasapas/>Java pas à pas</a></li><li><a href=/inf1220-hugo/docs/modules/module3/github/>GitHub</a></li><li><a href=/inf1220-hugo/docs/modules/module3/activite-3-1/>Les structures de contrôle</a></li><li><a href=/inf1220-hugo/docs/modules/module3/activite-3-2/>Les structures itératives</a></li><li><a href=/inf1220-hugo/docs/modules/module3/activite-3-3/>Les structures de données de base</a></li><li><a href=/inf1220-hugo/docs/modules/module3/fonctionnel/>La programmation fonctionnelle en Java</a></li><li><a href=/inf1220-hugo/docs/modules/module3/exercices-3-1/>Exercices sur les structures de contrôle, les structures de données, les structures itératives</a></li><li><a href=/inf1220-hugo/docs/modules/module3/activite-3-4/>Les exceptions</a></li><li><a href=/inf1220-hugo/docs/modules/module3/activite-3-5/>La récursivité</a></li><li><a href=/inf1220-hugo/docs/modules/module3/exercices-3-2/>Exercices sur les exceptions et la récursivité</a></li><li><a href=/inf1220-hugo/docs/modules/module3/conseils/>Recommandations</a></li><li><a href=/inf1220-hugo/docs/modules/module3/travail-note-3/>Travail noté 3</a></li></ul></li><li><input type=checkbox id=section-9087a4ffddbd9e1f0f2f4b3f8c85dcb4 class=toggle>
<label for=section-9087a4ffddbd9e1f0f2f4b3f8c85dcb4 class=flex><a href=/inf1220-hugo/docs/modules/module4/ class=flex-auto>Module 4: Les entrées et sorties</a></label><ul><li><a href=/inf1220-hugo/docs/modules/module4/pasapas/>Java pas à pas</a></li><li><a href=/inf1220-hugo/docs/modules/module4/activite-4-1/>Les flux de console</a></li><li><a href=/inf1220-hugo/docs/modules/module4/activite-4-2/>Les flux de données: lecture dans des fichiers et autres</a></li><li><a href=/inf1220-hugo/docs/modules/module4/exercices-4-1/>Exercices sur les flux</a></li><li><a href=/inf1220-hugo/docs/modules/module4/web/>Développement web</a></li><li><a href=/inf1220-hugo/docs/modules/module4/travail-note-4/>Travail noté 4</a></li></ul></li><li><input type=checkbox id=section-749269b787e65f29f734b50b6282d284 class=toggle>
<label for=section-749269b787e65f29f734b50b6282d284 class=flex><a href=/inf1220-hugo/docs/modules/module5/ class=flex-auto>Module 5. La programmation orientée objet: héritage et le polymorphisme</a></label><ul><li><a href=/inf1220-hugo/docs/modules/module5/pasapas/>Java pas à pas</a></li><li><a href=/inf1220-hugo/docs/modules/module5/activite-5-1/>L'héritage, les classes abstraites et les interfaces</a></li><li><a href=/inf1220-hugo/docs/modules/module5/activite-5-2/>Le polymorphisme</a></li><li><a href=/inf1220-hugo/docs/modules/module5/exercices-5-1/>Exercices sur l’héritage et le polymorphisme</a></li><li><a href=/inf1220-hugo/docs/modules/module5/travail-note-5/>Travail noté 5</a></li></ul></li><li><a href=/inf1220-hugo/docs/modules/examen/>Examen</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/inf1220-hugo/svg/menu.svg class=book-icon alt=Menu></label><h3>Robot conversationnel et intelligence artificielle</h3><label for=toc-control><img src=/inf1220-hugo/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#robot>Robot</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=robot-conversationnel-et-intelligence-artificielle>Robot conversationnel et intelligence artificielle
<a class=anchor href=#robot-conversationnel-et-intelligence-artificielle>#</a></h1><p>L&rsquo;intelligence artificielle (IA) transforme profondément le domaine de la programmation, redéfinissant la manière dont les développeurs conçoivent, écrivent et maintiennent le code. Les outils basés sur l&rsquo;IA, comme les assistants de codage (par exemple, GitHub Copilot), permettent d&rsquo;automatiser des tâches répétitives, telles que la génération de code boilerplate ou la complétion automatique de fonctions. Ces outils s&rsquo;appuient sur des modèles de langage avancés, entraînés sur d&rsquo;immenses bases de données de code, pour proposer des suggestions contextuelles précises. Cette assistance accélère le processus de développement, permettant aux programmeurs de se concentrer sur des aspects plus créatifs et complexes de leurs projets, tout en réduisant les erreurs humaines.</p><p>Au-delà de l&rsquo;écriture de code, l&rsquo;IA joue un rôle croissant dans le débogage et l&rsquo;optimisation. Les systèmes d&rsquo;IA peuvent analyser des programmes pour identifier des bugs, des vulnérabilités de sécurité ou des inefficacités, souvent plus rapidement qu&rsquo;un humain. Par exemple, des outils comme DeepCode ou SonarQube utilisent l&rsquo;apprentissage automatique pour détecter des anomalies dans le code et suggérer des corrections. De plus, l&rsquo;IA aide à optimiser les performances en proposant des algorithmes plus efficaces ou en ajustant automatiquement les configurations des systèmes. Cette capacité à diagnostiquer et améliorer le code renforce la qualité des logiciels et réduit le temps consacré à la maintenance.</p><p>L&rsquo;IA démocratise également la programmation en abaissant les barrières à l&rsquo;entrée. Les interfaces conversationnelles et les outils no-code/low-code, alimentés par l&rsquo;IA, permettent à des non-programmeurs de créer des applications en décrivant leurs besoins en langage naturel. Des plateformes comme Bubble ou OutSystems exploitent l&rsquo;IA pour traduire ces descriptions en code fonctionnel. Cette évolution ouvre la programmation à un public plus large, favorisant l&rsquo;innovation dans des domaines variés, mais soulève aussi des questions sur la dépendance aux outils automatisés et la compréhension réelle des concepts sous-jacents par les utilisateurs.</p><p>Je vous invite à regarder cette vidéo sur YouTube à ce sujet. YouTube offre une version
avec sous-titres traduits et je vous offre une transcription en français.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/LCEmiRjPEtQ?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><details><summary>Transcription traduite en français</summary><p>Hum, d&rsquo;accord, je suis enthousiaste d&rsquo;être ici aujourd&rsquo;hui pour vous parler du logiciel à l&rsquo;ère de l&rsquo;IA. On m&rsquo;a dit que beaucoup d&rsquo;entre vous sont étudiants, en licence, master, doctorat, et ainsi de suite, et que vous êtes sur le point d&rsquo;entrer dans l&rsquo;industrie. Je pense que c&rsquo;est un moment extrêmement unique et très intéressant pour rejoindre l&rsquo;industrie en ce moment. Fondamentalement, la raison en est que le logiciel change à nouveau. Je dis « à nouveau » parce que j&rsquo;ai déjà donné cette conférence, mais le problème est que le logiciel ne cesse de changer. J&rsquo;ai donc beaucoup de matériel pour créer de nouvelles conférences, et je pense que ce changement est assez fondamental. Grossièrement, le logiciel n&rsquo;a pas beaucoup changé à un niveau aussi fondamental depuis 70 ans, puis il a changé, je pense, deux fois de manière assez rapide ces dernières années. Il y a donc une énorme quantité de travail à faire, une énorme quantité de logiciels à écrire et à réécrire.</p><p>Le paysage du logiciel</p><p>Examinons peut-être le domaine du logiciel. Si nous considérons cela comme une carte du logiciel, il existe un outil vraiment cool appelé « Map of GitHub ». C&rsquo;est un peu comme tout le logiciel qui a été écrit, des instructions pour l&rsquo;ordinateur afin d&rsquo;exécuter des tâches dans l&rsquo;espace numérique. Si vous zoomez, ce sont tous différents types de dépôts, et c&rsquo;est tout le code qui a été écrit. Il y a quelques années, j&rsquo;ai observé que le logiciel changeait, qu&rsquo;il y avait un nouveau type de logiciel, et je l&rsquo;ai appelé « logiciel 2.0 » à l&rsquo;époque. L&rsquo;idée était que le logiciel 1.0 est le code que vous écrivez pour l&rsquo;ordinateur, tandis que le logiciel 2.0 concerne les réseaux neuronaux, en particulier les poids d&rsquo;un réseau neuronal. Vous n&rsquo;écrivez pas ce code directement, vous ajustez plutôt les ensembles de données, puis vous exécutez un optimiseur pour créer les paramètres de ce réseau neuronal. À l&rsquo;époque, les réseaux neuronaux étaient perçus comme un simple classificateur différent, comme un arbre de décision ou quelque chose comme ça. Je pense que ce cadre était beaucoup plus approprié.</p><p>Maintenant, nous avons l&rsquo;équivalent de GitHub dans le domaine du logiciel 2.0. Je pense que Hugging Face est fondamentalement l&rsquo;équivalent de GitHub pour le logiciel 2.0. Il y a aussi Model Atlas, où vous pouvez visualiser tout le code écrit, si vous êtes curieux. D&rsquo;ailleurs, le grand cercle au centre représente les paramètres de Flux, le générateur d&rsquo;images. Chaque fois que quelqu&rsquo;un ajuste un modèle au-dessus de Flux, vous créez en quelque sorte un « commit » dans cet espace, et vous obtenez un générateur d&rsquo;images différent.</p><p>En résumé, le logiciel 1.0 est le code informatique qui programme un ordinateur, le logiciel 2.0 sont les poids qui programment les réseaux neuronaux. Voici un exemple avec AlexNet, un réseau neuronal de reconnaissance d&rsquo;images. Jusqu&rsquo;à récemment, tous les réseaux neuronaux que nous connaissions étaient des ordinateurs à fonction fixe, comme de l&rsquo;image aux catégories. Ce qui a changé, et je pense que c&rsquo;est un changement fondamental, c&rsquo;est que les réseaux neuronaux sont devenus programmables avec les grands modèles de langage (LLM). Je vois cela comme quelque chose de nouveau et unique, un nouveau type d&rsquo;ordinateur. À mon avis, cela mérite une nouvelle désignation : le logiciel 3.0. Vos invites (prompts) sont maintenant des programmes qui programment le LLM, et, chose remarquable, ces invites sont écrites en anglais, ce qui en fait un langage de programmation très intéressant.</p><p>Exemple : classification de sentiments</p><p>Pour illustrer la différence, si vous faites une classification de sentiments, vous pouvez imaginer écrire une certaine quantité de code Python pour effectuer cette classification, ou entraîner un réseau neuronal, ou encore utiliser une invite pour un grand modèle de langage. Voici une invite courte, et vous pouvez imaginer la modifier pour programmer l&rsquo;ordinateur d&rsquo;une manière légèrement différente. Nous avons donc le logiciel 1.0, le logiciel 2.0, et je pense que nous voyons maintenant que beaucoup de code sur GitHub n&rsquo;est plus seulement du code, il y a aussi beaucoup de texte en anglais entrelacé avec le code. Une nouvelle catégorie de code émerge, non seulement un nouveau paradigme de programmation, mais aussi, ce qui est remarquable, dans notre langue native, l&rsquo;anglais.</p><p>Quand cela m&rsquo;a frappé il y a quelques années, j&rsquo;ai tweeté à ce sujet, et cela a capté l&rsquo;attention de beaucoup de monde. C&rsquo;est actuellement mon tweet épinglé : nous programmons maintenant les ordinateurs en anglais. Chez Tesla, nous travaillions sur le pilote automatique, et nous essayions de faire conduire la voiture. J&rsquo;ai montré une diapositive à l&rsquo;époque où les entrées de la voiture passaient par une pile logicielle pour produire la direction et l&rsquo;accélération. J&rsquo;avais observé qu&rsquo;il y avait une tonne de code C++ dans le pilote automatique, qui était du logiciel 1.0, et qu&rsquo;il y avait aussi des réseaux neuronaux pour la reconnaissance d&rsquo;images. Au fil du temps, à mesure que nous améliorions le pilote automatique, le réseau neuronal gagnait en capacité et en taille, et tout le code C++ était supprimé. Beaucoup des capacités et fonctionnalités initialement écrites en 1.0 ont été migrées vers le 2.0. Par exemple, l&rsquo;assemblage des informations entre les images des différentes caméras et dans le temps était effectué par un réseau neuronal, ce qui nous a permis de supprimer beaucoup de code. La pile logicielle 2.0 a littéralement dévoré la pile logicielle du pilote automatique.</p><p>Je trouvais cela vraiment remarquable à l&rsquo;époque, et je pense que nous voyons la même chose aujourd&rsquo;hui, où un nouveau type de logiciel dévore la pile. Nous avons trois paradigmes de programmation complètement différents, et si vous entrez dans l&rsquo;industrie, il est très utile d&rsquo;être à l&rsquo;aise avec chacun d&rsquo;eux, car ils ont tous leurs avantages et inconvénients. Vous devrez décider si une fonctionnalité doit être programmée en 1.0, 2.0 ou 3.0. Allez-vous entraîner un réseau neuronal, simplement utiliser une invite pour un LLM, ou écrire un code explicite ? Nous devons tous prendre ces décisions et potentiellement passer fluidement d&rsquo;un paradigme à l&rsquo;autre.</p><p>Les grands modèles de langage (LLM)</p><p>Passons maintenant à la première partie, où je veux parler des LLM, de la manière de penser à ce nouveau paradigme et à son écosystème. Qu&rsquo;est-ce que cet nouvel ordinateur, à quoi ressemble-t-il, et à quoi ressemble l&rsquo;écosystème ? J&rsquo;ai été frappé par une citation d&rsquo;Andrew Ng, il y a plusieurs années, qui disait que l&rsquo;IA est la nouvelle électricité. Je pense que cela capture quelque chose de très intéressant, car les LLM ont actuellement des propriétés d&rsquo;utilité publique. Les laboratoires de LLM, comme OpenAI, Gemini, Anthropic, etc., investissent des capitaux pour entraîner les LLM, ce qui équivaut à construire un réseau. Ensuite, il y a des dépenses opérationnelles pour fournir cette intelligence via des API à nous tous, à travers un accès mesuré où nous payons par million de jetons ou quelque chose comme ça. Nous avons beaucoup d&rsquo;exigences similaires à celles d&rsquo;une utilité publique : faible latence, haute disponibilité, qualité constante, etc.</p><p>Dans l&rsquo;électricité, vous auriez un commutateur de transfert pour passer de la grille à l&rsquo;énergie solaire, une batterie ou un générateur. Dans les LLM, nous avons peut-être OpenRouter, qui permet de basculer facilement entre différents types de LLM existants. Comme les LLM sont des logiciels, ils ne rivalisent pas pour l&rsquo;espace physique, donc il est acceptable d&rsquo;avoir, disons, six fournisseurs d&rsquo;électricité, et vous pouvez passer de l&rsquo;un à l&rsquo;autre, car ils ne concurrencent pas de manière aussi directe. Ce qui est aussi fascinant, c&rsquo;est que récemment, plusieurs LLM ont connu des pannes, et les gens se sont retrouvés bloqués, incapables de travailler. Quand les LLM de pointe tombent en panne, c&rsquo;est comme une baisse d&rsquo;intelligence dans le monde, un peu comme une tension instable dans le réseau, et la planète devient simplement moins intelligente. Plus nous dépendons de ces modèles, ce qui est déjà dramatique, plus cela va croître.</p><p>Mais les LLM n&rsquo;ont pas seulement des propriétés d&rsquo;utilité publique. Ils ont aussi des propriétés de fabriques (fabs), car les investissements nécessaires pour construire un LLM sont considérables, pas seulement comme construire une centrale électrique. La technologie évolue rapidement, avec des arbres technologiques complexes, de la recherche et du développement, et des secrets centralisés dans les laboratoires de LLM. Cependant, l&rsquo;analogie devient un peu floue, car, comme je l&rsquo;ai mentionné, il s&rsquo;agit de logiciel, et le logiciel est moins défendable car il est très malléable.</p><p>Je pense que l&rsquo;analogie qui a le plus de sens est que les LLM ont de fortes similitudes avec les systèmes d&rsquo;exploitation. Ce n&rsquo;est pas juste de l&rsquo;électricité ou de l&rsquo;eau qui sort d&rsquo;un robinet comme une commodité. Ce sont des écosystèmes logiciels de plus en plus complexes, pas juste des commodités simples comme l&rsquo;électricité. L&rsquo;écosystème se forme de manière très similaire, avec quelques fournisseurs à source fermée, comme Windows ou Mac OS, et une alternative open source comme Linux. Pour les LLM, nous avons quelques fournisseurs à source fermée en compétition, et peut-être que l&rsquo;écosystème LLaMA est actuellement une approximation de quelque chose qui pourrait devenir comme Linux. C&rsquo;est encore très tôt, car ce ne sont que des LLM simples, mais nous commençons à voir qu&rsquo;ils vont devenir beaucoup plus compliqués, avec l&rsquo;utilisation d&rsquo;outils, la multimodalité, et comment tout cela fonctionne.</p><p>Quand j&rsquo;ai réalisé cela il y a un moment, j&rsquo;ai essayé de le schématiser, et il m&rsquo;a semblé que les LLM sont comme un nouveau système d&rsquo;exploitation. Le LLM est une sorte d&rsquo;équivalent du CPU, les fenêtres de contexte sont comme la mémoire, et le LLM orchestre la mémoire et le calcul pour résoudre des problèmes, en utilisant toutes ces capacités. Cela ressemble beaucoup à un système d&rsquo;exploitation de ce point de vue.</p><p>Pour donner un exemple, si je veux télécharger une application, disons VS Code, je peux le télécharger et l&rsquo;exécuter sur Windows, Linux ou Mac. De la même manière, je peux prendre une application LLM comme Cursor et l&rsquo;exécuter sur GPT, Claude ou la série Gemini, juste en sélectionnant une option dans un menu déroulant. Nous sommes dans une ère, disons des années 1960, où le calcul des LLM est encore très coûteux pour ce nouveau type d&rsquo;ordinateur, ce qui oblige les LLM à être centralisés dans le cloud. Nous sommes tous des clients qui interagissent avec eux via le réseau, et aucun de nous n&rsquo;a une utilisation complète de ces ordinateurs. Cela rend logique d&rsquo;utiliser le partage de temps, où nous sommes tous une dimension du lot quand ils exécutent l&rsquo;ordinateur dans le cloud. C&rsquo;est ainsi que les ordinateurs fonctionnaient à cette époque : les systèmes d&rsquo;exploitation étaient dans le cloud, tout était diffusé, et il y avait du traitement par lots.</p><p>La révolution de l&rsquo;informatique personnelle n&rsquo;a pas encore eu lieu, car ce n&rsquo;est pas économique, ça n&rsquo;a pas de sens. Mais certaines personnes essaient, et il s&rsquo;avère que les Mac Minis, par exemple, sont très adaptés pour certains LLM, car si vous faites une inférence par lot, tout est très limité par la mémoire. Cela fonctionne, et ce sont peut-être des signes précoces de l&rsquo;informatique personnelle, mais cela n&rsquo;a pas vraiment eu lieu. Ce n&rsquo;est pas clair à quoi cela ressemblera. Peut-être que certains d&rsquo;entre vous inventeront ce que c&rsquo;est, comment ça fonctionne, ou ce que ça devrait être.</p><p>Une autre analogie : chaque fois que je parle à ChatGPT ou à un LLM directement en texte, j&rsquo;ai l&rsquo;impression de parler à un système d&rsquo;exploitation via le terminal. C&rsquo;est du texte, c&rsquo;est un accès direct au système d&rsquo;exploitation. Une interface graphique (GUI) n&rsquo;a pas encore été inventée de manière générale. Est-ce que ChatGPT devrait avoir une GUI différente des simples bulles de texte ? Certaines applications ont des GUI, mais il n&rsquo;y a pas de GUI générale pour toutes les tâches.</p><p>Les LLM diffèrent des systèmes d&rsquo;exploitation de manière assez unique. J&rsquo;ai écrit sur une propriété qui me semble très différente cette fois-ci : les LLM inversent la direction de la diffusion technologique, qui est généralement présente dans la technologie. Par exemple, avec l&rsquo;électricité, la cryptographie, l&rsquo;informatique, l&rsquo;aviation, l&rsquo;internet, le GPS, beaucoup de technologies transformatrices nouvelles et coûteuses étaient d&rsquo;abord utilisées par les gouvernements et les entreprises, avant de se diffuser aux consommateurs. Mais avec les LLM, c&rsquo;est l&rsquo;inverse. Avec les premiers ordinateurs, il s&rsquo;agissait de balistique et d&rsquo;usage militaire, mais avec les LLM, il s&rsquo;agit de savoir comment faire bouillir un œuf. C&rsquo;est fascinant que nous ayons un nouvel ordinateur magique qui m&rsquo;aide à faire bouillir un œuf, et non à aider le gouvernement à faire quelque chose de fou comme de la balistique militaire ou une technologie spéciale. Les entreprises et les gouvernements sont en retard sur l&rsquo;adoption de ces technologies par rapport à nous tous. Cela informe peut-être certaines utilisations de la technologie, comme où se trouvent les premières applications.</p><p>Résumé</p><p>Les LLM sont des systèmes d&rsquo;exploitation complexes, comparables à l&rsquo;informatique des années 1960, et nous refaisons l&rsquo;informatique à nouveau. Ils sont actuellement disponibles via le partage de temps et distribués comme une utilité publique. Ce qui est nouveau et sans précédent, c&rsquo;est qu&rsquo;ils ne sont pas entre les mains de quelques gouvernements et entreprises, mais entre les mains de nous tous, car nous avons tous un ordinateur, et c&rsquo;est juste du logiciel. ChatGPT a été envoyé à nos ordinateurs, à des milliards de personnes, instantanément et du jour au lendemain, ce qui est insensé. Maintenant, c&rsquo;est à nous d&rsquo;entrer dans l&rsquo;industrie et de programmer ces ordinateurs. C&rsquo;est assez remarquable.</p><p>Psychologie des LLM</p><p>Avant de programmer les LLM, nous devons réfléchir à ce qu&rsquo;ils sont. J&rsquo;aime parler de leur psychologie. Je vois les LLM comme des esprits humains, des simulations stochastiques de personnes, où le simulateur est un transformateur autorégressif. C&rsquo;est un réseau neuronal qui avance token par token, avec presque la même quantité de calcul pour chaque token. Ce simulateur est ajusté à tout le texte que nous avons sur internet, et ainsi de suite, ce qui lui donne une psychologie émergente semblable à celle des humains.</p><p>La première chose que vous remarquez, c&rsquo;est que les LLM ont une connaissance encyclopédique et une mémoire impressionnante. Ils peuvent se souvenir de beaucoup plus de choses qu&rsquo;un individu humain, car ils ont lu énormément. Cela me rappelle le film Rain Man, que je recommande vivement. Dustin Hoffman y joue un savant autiste avec une mémoire presque parfaite, capable de lire un annuaire téléphonique et de se souvenir de tous les noms et numéros. Les LLM sont similaires : ils peuvent se souvenir de hachages SHA et de toutes sortes de choses très facilement. Ils ont donc des superpouvoirs à certains égards.</p><p>Mais ils ont aussi des déficits cognitifs. Ils hallucinent beaucoup, inventent des choses, et n&rsquo;ont pas un très bon modèle interne de connaissance de soi, bien que cela s&rsquo;améliore. Ils affichent une intelligence inégale : ils sont surhumains dans certains domaines de résolution de problèmes, mais font des erreurs qu&rsquo;aucun humain ne ferait, comme insister que 9.11 est supérieur à 9.9 ou qu&rsquo;il y a deux « r » dans « strawberry ». Ce sont des exemples célèbres, mais il y a des aspérités sur lesquelles on peut trébucher.</p><p>Ils souffrent aussi d&rsquo;une sorte d&rsquo;amnésie rétrograde. Si un collègue rejoint votre organisation, il apprendra au fil du temps à connaître l&rsquo;organisation, gagnera du contexte, rentrera chez lui, dormira, consolidera ses connaissances et développera une expertise. Les LLM ne le font pas nativement, et ce n&rsquo;est pas quelque chose qui a été résolu dans la recherche et développement des LLM. Les fenêtres de contexte sont comme une mémoire de travail, et vous devez programmer cette mémoire de travail assez directement, car ils ne deviennent pas plus intelligents par défaut. Beaucoup de gens se trompent sur ces analogies. Je recommande de regarder les films Memento et 50 First Dates, où les protagonistes ont leurs poids fixés et leurs fenêtres de contexte effacées chaque matin, ce qui rend le travail ou les relations problématiques.</p><p>Il y a aussi des limitations liées à la sécurité. Les LLM sont assez crédules, vulnérables aux risques d&rsquo;injection de prompts, et peuvent divulguer vos données. Il y a donc de nombreuses considérations liées à la sécurité.</p><p>En résumé, vous devez considérer cette chose surhumaine avec des déficits cognitifs et des problèmes, tout en étant extrêmement utile. Comment les programmer et contourner leurs déficits tout en profitant de leurs superpouvoirs ?</p><p>Opportunités avec les LLM</p><p>Passons maintenant aux opportunités d&rsquo;utilisation de ces modèles et à certaines des plus grandes opportunités. Ce n&rsquo;est pas une liste exhaustive, juste quelques éléments que je trouve intéressants pour cette conférence.</p><p>Applications à autonomie partielle</p><p>Je suis enthousiaste à propos de ce que j&rsquo;appelle les applications à autonomie partielle. Prenons l&rsquo;exemple du codage. Vous pouvez aller directement sur ChatGPT, copier-coller du code, des rapports de bogues, obtenir du code et tout copier-coller. Pourquoi faire cela ? Pourquoi aller directement au système d&rsquo;exploitation ? Il est beaucoup plus logique d&rsquo;avoir une application dédiée. Beaucoup d&rsquo;entre vous utilisent probablement Cursor, que j&rsquo;utilise aussi. Cursor est un très bon exemple d&rsquo;une application LLM précoce avec des propriétés utiles pour toutes les applications LLM.</p><p>Vous remarquerez que nous avons une interface traditionnelle qui permet à un humain de faire tout le travail manuellement comme avant, mais en plus, nous avons cette intégration LLM qui permet d&rsquo;avancer par plus gros morceaux. Voici quelques propriétés des applications LLM que je trouve utiles à souligner :</p><p>Les LLM gèrent une grande partie de la gestion du contexte.</p><p>Ils orchestrent plusieurs appels aux LLM. Dans le cas de Cursor, il y a des modèles d&rsquo;embedding pour tous vos fichiers, des modèles de chat, des modèles qui appliquent des différences au code, et tout cela est orchestré pour vous.</p><p>Une interface graphique spécifique à l&rsquo;application est très importante. Vous ne voulez pas parler directement au système d&rsquo;exploitation en texte. Le texte est difficile à lire, interpréter et comprendre, et vous ne voulez pas prendre certaines actions nativement en texte. Il est beaucoup plus facile de voir une différence en rouge et vert, de voir ce qui est ajouté ou soustrait, et d&rsquo;utiliser des commandes comme Cmd+Y pour accepter ou Cmd+N pour rejeter, plutôt que de devoir l&rsquo;écrire en texte. Une interface graphique permet à un humain d&rsquo;auditer le travail de ces systèmes faillibles et d&rsquo;aller plus vite.</p><p>Ce que j&rsquo;appelle le curseur d&rsquo;autonomie. Dans Cursor, vous pouvez faire une complétion par tabulation, où vous êtes principalement en charge. Vous pouvez sélectionner un morceau de code et utiliser Cmd+K pour modifier juste ce morceau, Cmd+L pour modifier tout le fichier, ou Cmd+I pour laisser l&rsquo;application faire ce qu&rsquo;elle veut dans tout le dépôt, ce qui est la version agentique à pleine autonomie. Vous contrôlez ce curseur d&rsquo;autonomie, et selon la complexité de la tâche, vous pouvez ajuster le niveau d&rsquo;autonomie que vous êtes prêt à céder.</p><p>Un autre exemple d&rsquo;application LLM réussie est Perplexity. Elle possède des fonctionnalités similaires à celles que j&rsquo;ai mentionnées pour Cursor. Elle regroupe beaucoup d&rsquo;informations, orchestre plusieurs LLM, et a une interface graphique qui permet d&rsquo;auditer une partie de son travail, comme citer des sources que vous pouvez inspecter. Elle a aussi un curseur d&rsquo;autonomie : vous pouvez faire une recherche rapide, une recherche approfondie, ou une recherche très approfondie et revenir 10 minutes plus tard. Ce sont différents niveaux d&rsquo;autonomie que vous cédez à l&rsquo;outil.</p><p>Je me demande à quoi cela ressemble si beaucoup de logiciels deviennent partiellement autonomes. Pour ceux d&rsquo;entre vous qui maintiennent des produits et services, comment allez-vous rendre vos produits et services partiellement autonomes ? Un LLM peut-il voir tout ce qu&rsquo;un humain peut voir ? Un LLM peut-il agir de toutes les manières dont un humain pourrait agir ? Les humains peuvent-ils superviser et rester dans la boucle de cette activité, car ce sont des systèmes faillibles qui ne sont pas encore parfaits ? À quoi ressemble une différence dans Photoshop, par exemple ? Beaucoup de logiciels traditionnels ont actuellement des interrupteurs et des éléments conçus pour les humains. Tout cela doit changer et devenir accessible aux LLM.</p><p>Un point que je veux souligner avec beaucoup de ces applications LLM, qui ne reçoit peut-être pas autant d&rsquo;attention qu&rsquo;il le devrait, est que nous coopérons maintenant avec des IA. Habituellement, elles génèrent, et nous, humains, vérifions. Il est dans notre intérêt de faire tourner cette boucle le plus rapidement possible pour accomplir beaucoup de travail. Il y a deux façons principales d&rsquo;y parvenir :</p><p>Accélérer la vérification. Les interfaces graphiques sont extrêmement importantes pour cela, car elles exploitent le GPU de votre vision par ordinateur dans votre tête. Lire du texte est laborieux et pas amusant, mais regarder des choses est amusant et constitue une autoroute vers votre cerveau. Les interfaces graphiques sont donc très utiles pour auditer les systèmes et pour les représentations visuelles en général.</p><p>Garder l&rsquo;IA en laisse. Beaucoup de gens s&rsquo;emballent trop avec les agents IA. Ce n&rsquo;est pas utile de recevoir une différence de 10 000 lignes de code dans mon dépôt. Je reste le goulot d&rsquo;étranglement, même si ces 10 000 lignes sortent instantanément. Je dois m&rsquo;assurer que cela n&rsquo;introduit pas de bogues, que c&rsquo;est correct, et qu&rsquo;il n&rsquo;y a pas de problèmes de sécurité. Il est dans notre intérêt de faire tourner ce flux très rapidement et de garder l&rsquo;IA en laisse, car elle devient trop réactive.</p><p>Quand je fais du codage assisté par IA, si je code tranquillement, tout va bien, mais si j&rsquo;essaie d&rsquo;avancer dans mon travail, ce n&rsquo;est pas génial d&rsquo;avoir un agent trop réactif qui fait tout. Je travaille toujours sur de petits morceaux incrémentiels, je veux m&rsquo;assurer que tout va bien, je veux faire tourner cette boucle très rapidement, et je travaille sur des choses concrètes et uniques. Beaucoup d&rsquo;entre vous développent probablement des façons similaires de travailler avec les LLM. J&rsquo;ai aussi vu plusieurs articles de blog qui tentent de développer ces meilleures pratiques pour travailler avec les LLM. J&rsquo;en ai lu un récemment qui était assez bon, discutant de certaines techniques, notamment sur la manière de garder l&rsquo;IA en laisse. Par exemple, si votre invite est vague, l&rsquo;IA pourrait ne pas faire exactement ce que vous vouliez, et la vérification échouera. Vous demanderez autre chose, et si la vérification échoue, vous commencerez à tourner en rond. Il est donc plus logique de passer un peu plus de temps à être plus précis dans vos invites, ce qui augmente la probabilité d&rsquo;une vérification réussie, et vous pouvez avancer.</p><p>Dans mon propre travail, je m&rsquo;intéresse actuellement à ce que l&rsquo;éducation pourrait être avec l&rsquo;IA et les LLM. Beaucoup de mes réflexions portent sur la manière de garder l&rsquo;IA en laisse. Je ne pense pas que cela fonctionne de dire à ChatGPT « Hey, enseigne-moi la physique ». L&rsquo;IA se perd dans les bois. Pour moi, ce sont deux applications distinctes : une pour un enseignant qui crée des cours, et une qui prend ces cours et les sert aux étudiants. Dans les deux cas, nous avons cet artefact intermédiaire d&rsquo;un cours qui est auditable, nous pouvons nous assurer qu&rsquo;il est bon, cohérent, et l&rsquo;IA est gardée en laisse par rapport à un certain programme, une certaine progression de projets, etc. C&rsquo;est une façon de garder l&rsquo;IA en laisse, et je pense que cela a beaucoup plus de chances de fonctionner.</p><p>Une autre analogie à laquelle je fais référence est mon expérience chez Tesla, où j&rsquo;ai travaillé pendant cinq ans sur un produit à autonomie partielle, qui partage beaucoup de caractéristiques. Par exemple, dans le tableau de bord, il y a l&rsquo;interface graphique du pilote automatique, qui montre ce que le réseau neuronal voit. Nous avions le curseur d&rsquo;autonomie, et au fil de mon temps là-bas, nous faisions de plus en plus de tâches autonomes pour l&rsquo;utilisateur. Une petite histoire : la première fois que j&rsquo;ai conduit un véhicule autonome, c&rsquo;était en 2013. Un ami qui travaillait chez Waymo m&rsquo;a proposé de faire un tour à Palo Alto. J&rsquo;ai pris une photo avec Google Glass à l&rsquo;époque – beaucoup d&rsquo;entre vous sont si jeunes que vous ne savez peut-être même pas ce que c&rsquo;est. Nous sommes montés dans la voiture, avons fait un trajet d&rsquo;environ 30 minutes sur les autoroutes et les rues de Palo Alto, et ce trajet était parfait, sans aucune intervention. C&rsquo;était en 2013, il y a 12 ans, et cela m&rsquo;a frappé, car à l&rsquo;époque, après ce trajet parfait, j&rsquo;ai pensé que la conduite autonome était imminente. Mais nous sommes en 2025, et nous travaillons toujours sur l&rsquo;autonomie, sur les agents de conduite. Même maintenant, nous n&rsquo;avons pas vraiment résolu le problème. Vous voyez peut-être des Waymo circuler sans conducteur, mais il y a encore beaucoup de téléopération et d&rsquo;humains dans la boucle. Nous n&rsquo;avons pas encore déclaré le succès, mais je pense que cela va réussir à ce stade, mais cela a pris beaucoup de temps.</p><p>Le logiciel est vraiment complexe, tout comme la conduite. Quand je vois des affirmations comme « 2025 est l&rsquo;année des agents », je m&rsquo;inquiète. Je pense que c&rsquo;est la décennie des agents, et cela va prendre du temps. Nous avons besoin d&rsquo;humains dans la boucle, nous devons le faire prudemment. Soyons sérieux, c&rsquo;est du logiciel.</p><p>Une autre analogie que je considère toujours est l&rsquo;armure d&rsquo;Iron Man. J&rsquo;adore Iron Man, je pense que c&rsquo;est tellement juste à bien des égards concernant la technologie et comment elle va se déployer. Ce que j&rsquo;aime dans l&rsquo;armure d&rsquo;Iron Man, c&rsquo;est qu&rsquo;elle est à la fois une augmentation – Tony Stark peut la piloter – et un agent. Dans certains films, l&rsquo;armure est assez autonome, peut voler et trouver Tony, etc. C&rsquo;est le curseur d&rsquo;autonomie : nous pouvons construire des augmentations ou des agents, et nous voulons faire un peu des deux. Mais à ce stade, en travaillant avec des LLM faillibles, je dirais qu&rsquo;il s&rsquo;agit moins de robots Iron Man et plus de combinaisons Iron Man. Il s&rsquo;agit moins de construire des démos flashy d&rsquo;agents autonomes et plus de construire des produits à autonomie partielle. Ces produits ont des interfaces graphiques personnalisées et une expérience utilisateur, conçus pour que la boucle de génération-vérification humaine soit très rapide, tout en gardant à l&rsquo;esprit qu&rsquo;il est en principe possible d&rsquo;automatiser ce travail. Il devrait y avoir un curseur d&rsquo;autonomie dans votre produit, et vous devriez réfléchir à comment faire glisser ce curseur pour rendre votre produit plus autonome avec le temps.</p><p>Programmation en anglais</p><p>Passons à une autre dimension que je trouve très unique. Non seulement il y a un nouveau type de langage de programmation qui permet l&rsquo;autonomie dans les logiciels, mais comme je l&rsquo;ai mentionné, il est programmé en anglais, qui est une interface naturelle. Soudain, tout le monde est programmeur, car tout le monde parle une langue naturelle comme l&rsquo;anglais. C&rsquo;est extrêmement optimiste et très intéressant pour moi, et totalement sans précédent. Avant, il fallait passer cinq à dix ans à étudier pour pouvoir faire quelque chose en logiciel. Ce n&rsquo;est plus le cas.</p><p>Quelqu&rsquo;un a-t-il entendu parler du « vibe coding » ? C&rsquo;est un tweet qui a introduit ce concept, et on m&rsquo;a dit que c&rsquo;est maintenant un mème majeur. Une anecdote amusante : je suis sur Twitter depuis environ 15 ans, et je n&rsquo;ai toujours aucune idée de quel tweet va devenir viral et lequel va passer inaperçu. Je pensais que ce tweet allait être dans la deuxième catégorie, juste une pensée spontanée, mais il est devenu un mème total. Je ne peux pas vraiment prévoir, mais je suppose qu&rsquo;il a touché une corde sensible et donné un nom à quelque chose que tout le monde ressentait mais ne pouvait pas exprimer en mots. Maintenant, il y a même une page Wikipédia pour ça.</p><p>Tom Wolf de Hugging Face a partagé une vidéo magnifique que j&rsquo;adore, montrant des enfants en train de faire du vibe coding. Je trouve cette vidéo tellement saine. Comment peut-on regarder cette vidéo et se sentir mal à propos de l&rsquo;avenir ? L&rsquo;avenir est prometteur. Je pense que cela deviendra une porte d&rsquo;entrée vers le développement logiciel. Je ne suis pas pessimiste quant à l&rsquo;avenir de cette génération.</p><p>J&rsquo;ai aussi essayé le vibe coding, car c&rsquo;est tellement amusant. C&rsquo;est génial quand vous voulez construire quelque chose de super personnalisé qui n&rsquo;existe pas et que vous voulez juste tenter le coup un samedi. J&rsquo;ai construit une application iOS, et je ne sais pas programmer en Swift, mais j&rsquo;ai été choqué de pouvoir construire une application super basique. Je ne vais pas l&rsquo;expliquer, c&rsquo;est vraiment idiot, mais c&rsquo;était juste une journée de travail, et ça fonctionnait sur mon téléphone le même jour. J&rsquo;étais comme « Wow, c&rsquo;est incroyable ». Je n&rsquo;ai pas eu à lire des manuels sur Swift pendant cinq jours pour commencer.</p><p>J&rsquo;ai aussi fait du vibe coding pour une application appelée MenuGen, qui est en ligne sur menu.app. J&rsquo;avais ce problème où j&rsquo;arrive dans un restaurant, je lis le menu, et je n&rsquo;ai aucune idée de ce que sont les plats, et j&rsquo;ai besoin d&rsquo;images. Ça n&rsquo;existait pas, alors je me suis dit « Je vais le coder en mode vibe ». Vous allez sur menu.app, prenez une photo d&rsquo;un menu, et MenuGen génère les images. Tout le monde reçoit 5 $ de crédits gratuits en s&rsquo;inscrivant, ce qui est un centre de coûts majeur dans ma vie. Cette application me fait perdre beaucoup d&rsquo;argent.</p><p>Ce qui est fascinant avec MenuGen, c&rsquo;est que coder la partie vibe coding était la partie facile. La plupart des difficultés sont survenues quand j&rsquo;ai essayé de la rendre réelle, avec l&rsquo;authentification, les paiements, le nom de domaine, et le déploiement sur Vercel. Tout cela n&rsquo;était pas du code, c&rsquo;était du DevOps, cliquer sur des choses dans le navigateur, et c&rsquo;était extrêmement lent, ça a pris une autre semaine. J&rsquo;avais une démo de MenuGen fonctionnant sur mon ordinateur portable en quelques heures, mais il m&rsquo;a fallu une semaine pour la rendre réelle, car c&rsquo;était vraiment agaçant. Par exemple, ajouter une connexion Google à votre page web implique une énorme quantité d&rsquo;instructions d&rsquo;une bibliothèque comme Clerk, me disant d&rsquo;aller à telle URL, de cliquer sur tel menu déroulant, de choisir ceci, d&rsquo;aller là et de cliquer sur ça. C&rsquo;est comme si un ordinateur me disait quoi faire. Pourquoi est-ce que je fais ça ? Fais-le toi-même !</p><p>Construire pour les agents</p><p>La dernière partie de ma conférence se concentre sur la possibilité de construire pour les agents. Je ne veux pas faire ce travail, les agents peuvent-ils le faire ? Grossièrement, je pense qu&rsquo;il y a une nouvelle catégorie de consommateurs et de manipulateurs d&rsquo;informations numériques. Avant, c&rsquo;étaient juste les humains via les interfaces graphiques ou les ordinateurs via les API. Maintenant, nous avons une chose complètement nouvelle : les agents. Ce sont des ordinateurs, mais ils sont un peu comme des humains, des esprits humains sur internet, et ils doivent interagir avec notre infrastructure logicielle. Pouvons-nous construire pour eux ? C&rsquo;est une nouveauté.</p><p>Par exemple, vous pouvez avoir un fichier robots.txt sur votre domaine pour indiquer aux robots d&rsquo;indexation comment se comporter sur votre site. De la même manière, vous pourriez avoir un fichier llm.txt, un simple fichier Markdown expliquant à un LLM de quoi parle ce domaine. C&rsquo;est très lisible pour un LLM. S&rsquo;il devait récupérer le HTML de votre page web et essayer de le parser, ce serait très sujet aux erreurs et difficile. Nous pouvons parler directement aux LLM, ça vaut le coup.</p><p>Une grande quantité de documentation est actuellement écrite pour les humains, avec des listes, du texte en gras, des images, ce qui n&rsquo;est pas directement accessible aux LLM. Certains services commencent à transformer leurs documentations pour qu&rsquo;elles soient spécifiquement destinées aux LLM. Vercel et Stripe, par exemple, sont des pionniers ici, mais j&rsquo;en ai vu d&rsquo;autres. Ils proposent leur documentation en Markdown, qui est super facile à comprendre pour les LLM. C&rsquo;est génial.</p><p>Un exemple simple de mon expérience : certains d&rsquo;entre vous connaissent peut-être 3Blue1Brown, qui fait de magnifiques vidéos d&rsquo;animation sur YouTube. Il a écrit une bibliothèque appelée Manim, et je voulais faire mes propres animations. Il y a une documentation extensive sur l&rsquo;utilisation de Manim, mais je ne voulais pas la lire. J&rsquo;ai donc copié-collé tout le contenu dans un LLM, décrit ce que je voulais, et ça a fonctionné directement. Le LLM m&rsquo;a créé une animation exactement comme je le voulais, et j&rsquo;étais comme « Wow, c&rsquo;est incroyable ». Si nous rendons les documentations lisibles pour les LLM, cela va débloquer une énorme quantité d&rsquo;utilisations, et je pense que c&rsquo;est merveilleux et que ça devrait se faire plus souvent.</p><p>Malheureusement, il ne s&rsquo;agit pas seulement de prendre vos documentations et de les mettre en Markdown, ce qui est la partie facile. Il faut aussi modifier les documentations, car chaque fois qu&rsquo;elles disent « cliquez ici », c&rsquo;est mauvais. Un LLM ne peut pas nativement prendre cette action pour le moment. Vercel, par exemple, remplace chaque occurrence de « cliquez » par une commande curl équivalente que votre agent LLM pourrait exécuter à votre place. Je trouve cela très intéressant. Il y a aussi le protocole de contexte de modèle d&rsquo;Anthropic, une autre façon de parler directement aux agents en tant que nouveaux consommateurs et manipulateurs d&rsquo;informations numériques. Je suis très optimiste sur ces idées.</p><p>J&rsquo;aime aussi plusieurs petits outils ici et là qui aident à ingérer des données dans des formats très adaptés aux LLM. Par exemple, quand je vais sur un dépôt GitHub comme mon dépôt nanoGPT, je ne peux pas le donner à un LLM et poser des questions, car c&rsquo;est une interface humaine sur GitHub. Mais si vous changez l&rsquo;URL de GitHub à GetIngest, cela concatène tous les fichiers en un seul texte géant, crée une structure de répertoire, etc., et c&rsquo;est prêt à être copié-collé dans votre LLM préféré. Un exemple encore plus frappant est DeepWiki, où ce n&rsquo;est pas juste le contenu brut des fichiers. Devon, par exemple, analyse le dépôt GitHub et construit toute une page de documentation pour votre dépôt, ce qui est encore plus utile à copier-coller dans votre LLM.</p><p>J&rsquo;adore tous ces petits outils où vous changez simplement l&rsquo;URL, et ça rend quelque chose accessible à un LLM. C&rsquo;est très bien, et il devrait y en avoir beaucoup plus. Une note supplémentaire : il est tout à fait possible que dans le futur – et même aujourd&rsquo;hui – les LLM puissent naviguer et cliquer sur des choses. Mais je pense toujours qu&rsquo;il vaut la peine de rencontrer les LLM à mi-chemin et de faciliter leur accès à toutes ces informations, car c&rsquo;est encore assez coûteux et beaucoup plus difficile. Il y aura une longue traîne de logiciels qui ne s&rsquo;adapteront pas, car ce ne sont pas des dépôts ou des infrastructures numériques très actifs. Nous aurons besoin de ces outils, mais pour tous les autres, je pense qu&rsquo;il vaut la peine de trouver un point de rencontre.</p><p>Conclusion</p><p>Quel moment incroyable pour entrer dans l&rsquo;industrie ! Nous devons réécrire une tonne de code, qui sera écrit par des professionnels et des codeurs. Les LLM sont un peu comme des utilités publiques, un peu comme des fabriques, mais surtout comme des systèmes d&rsquo;exploitation. C&rsquo;est tellement tôt, c&rsquo;est comme les années 1960 des systèmes d&rsquo;exploitation, et beaucoup d&rsquo;analogies se croisent. Ces LLM sont comme des esprits humains faillibles avec lesquels nous devons apprendre à travailler. Pour le faire correctement, nous devons ajuster notre infrastructure en conséquence.</p><p>Quand vous construisez des applications LLM, j&rsquo;ai décrit certaines façons de travailler efficacement avec ces LLM et certains outils qui rendent cela possible, ainsi que comment faire tourner cette boucle très rapidement pour créer des produits à autonomie partielle. Beaucoup de code devra aussi être écrit plus directement pour les agents. En revenant à l&rsquo;analogie de l&rsquo;armure d&rsquo;Iron Man, je pense que sur la prochaine décennie, nous allons faire glisser le curseur d&rsquo;autonomie de gauche à droite, et il sera très intéressant de voir à quoi cela ressemble. J&rsquo;ai hâte de le construire avec vous tous. Merci.</p></details><p>Après avoir écouté la vidéo, je vous invite à réfléchir aux questions suivantes.</p><ol><li><p>Comment les paradigmes du logiciel 1.0, 2.0 et 3.0 diffèrent-ils dans leur approche de la programmation, et quelles implications cela a-t-il pour les choix que vous feriez en tant que développeur face à une tâche spécifique ?</p></li><li><p>En quoi l’analogie des grands modèles de langage (LLM) avec un système d’exploitation vous aide-t-elle à comprendre leur rôle dans le développement logiciel actuel, et quelles limites cette analogie pourrait-elle avoir ?</p></li><li><p>Quels problèmes éthiques ou pratiques pourraient surgir de l’utilisation croissante de la programmation en langage naturel, comme le « vibe coding », et comment les développeurs peuvent-ils y répondre pour garantir des applications fiables et accessibles ?</p></li></ol><h2 id=robot>Robot
<a class=anchor href=#robot>#</a></h2><p>Dans ce cours, vous êtes encouragé à utiliser l&rsquo;intelligence artificielle pour mieux apprendre
à programmer. <a href=https://rc-inf1220.teluq.ca/#>L&rsquo;Université TÉLUQ met à votre disposition un robot conversationnel dédié au cours</a>. Ce robot, basé sur des technologies avancées d&rsquo;IA, vous permettra d&rsquo;obtenir des réponses personnalisées à vos questions, de clarifier des concepts complexes et de recevoir des exemples de code pertinents. En interagissant avec cet outil, vous pourrez approfondir votre compréhension des notions abordées, pratiquer vos compétences en programmation et progresser à votre rythme, tout en bénéficiant d&rsquo;un soutien adapté à vos besoins. Vous pouvez aussi utiliser d&rsquo;autres outils comme ChatGTP, copilot, Grok, etc.</p><p>Le cours INF 1220 a probablement été le premier cours au Québec à disposer d&rsquo;un robot conversationnel. Je vous invite à consulter cet article par madame Roy de Radio-Canada:</p><ul><li><a href=https://ici.radio-canada.ca/nouvelle/2067576/teluq-universite-laval-ia-chatgpt-robot>IA à l’université : mieux comprendre pour mieux se préparer</a></li></ul><p>Pour maximiser l’efficacité de ces outils d’intelligence artificielle, il est recommandé de poser des questions précises et bien formulées. Par exemple, demandez des explications sur des erreurs de code spécifiques, des suggestions pour optimiser vos programmes ou des éclaircissements sur des concepts théoriques. Ces outils peuvent également vous aider à explorer des approches alternatives pour résoudre des problèmes de programmation, renforçant ainsi votre créativité et votre autonomie. En combinant l’utilisation de ces ressources avec les activités du cours, vous développerez des compétences solides et une meilleure confiance en vos capacités de programmation.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#robot>Robot</a></li></ul></nav></div></aside></main></body></html>